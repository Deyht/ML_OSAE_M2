{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ASIRRA TensorFlow**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Deyht/AI_astro_ED_AAIF/blob/main/codes/CNN/classification/ASIRRA_TensorFlow.ipynb)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "JfKCrIlDu-E0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ASIRRA**\n",
        "\n",
        "The ASIRRA (Animal Species Image Recognition for Restricting Access) is a dataset that was originally used for CAPTCHA and HIP (Human Interactive Proofs).\n",
        "\n",
        "The dataset comprises 25000 images of variable resolution (averaging around 350x500) and perfectly distributed over the two classes \"Cat\" and \"Dog\". For this course, we provide a resized to 128x128 and squared version of the dataset so it can fit into the limited amount of Colab RAM more easily."
      ],
      "metadata": {
        "id": "gd2waB3JYNkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Downloading and visualizing the data\n"
      ],
      "metadata": {
        "id": "kULtlVy8Y5UO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd /content/\n",
        "\n",
        "#Manually upload the directory to github if not yet opened\n",
        "git clone https://github.com/Deyht/AI_astro_ED_AAIF/"
      ],
      "metadata": {
        "id": "mjcrByRgYof3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd /content/AI_astro_ED_AAIF/codes/CNN/classification/ASIRRA\n",
        "\n",
        "python3 - <<EOF\n",
        "\n",
        "#Will download the dataset at the fist call\n",
        "from aux_fct import *\n",
        "\n",
        "init_data_gen(0)\n",
        "\n",
        "print(\"\\nOrdered validation examples\")\n",
        "create_val_batch()\n",
        "\n",
        "print(\"Create visualization of the validation dataset\")\n",
        "visual_val(8,4)\n",
        "\n",
        "EOF"
      ],
      "metadata": {
        "id": "-PsyIg0sZmSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AI_astro_ED_AAIF/codes/CNN/classification/ASIRRA\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "im = Image.open(\"val_mosaic.jpg\")\n",
        "plt.figure(figsize=(8,4), dpi=200)\n",
        "plt.imshow(im)\n",
        "plt.gca().axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xwExEJTXZygB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training a network\n",
        "\n"
      ],
      "metadata": {
        "id": "6ZeV1bvjRS4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AI_astro_ED_AAIF/codes/CNN/classification/ASIRRA\n",
        "\n",
        "import numpy as np\n",
        "from threading import Thread\n",
        "#from aux_fct import *\n",
        "import gc, os, sys, glob\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "\n",
        "class_count = 12500\n",
        "nb_class = 2\n",
        "\n",
        "nb_keep_val = 1024\n",
        "\n",
        "image_size_raw = 128\n",
        "image_size = 128\n",
        "#working image size can be lowered to increase computation speed\n",
        "\n",
        "batch_size = 16\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "6R3l1EKah4eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_array = np.reshape(np.fromfile(\"asirra_bin_128.dat\", dtype=\"uint8\"), (class_count*2,image_size_raw,image_size_raw,3))\n",
        "\n",
        "train_examples = np.append(raw_data_array[:class_count-nb_keep_val], raw_data_array[class_count:-nb_keep_val], axis=0)\n",
        "test_examples = np.append(raw_data_array[class_count-nb_keep_val:class_count], raw_data_array[-nb_keep_val:], axis=0)\n",
        "\n",
        "del(raw_data_array)\n",
        "gc.collect()\n",
        "\n",
        "train_labels = np.zeros((np.shape(train_examples)[0],nb_class))\n",
        "test_labels = np.zeros((np.shape(test_examples)[0],nb_class))\n",
        "\n",
        "train_labels[:class_count-nb_keep_val,0] = 1.0\n",
        "train_labels[class_count-nb_keep_val:,1] = 1.0\n",
        "\n",
        "test_labels[:nb_keep_val,0] = 1.0\n",
        "test_labels[nb_keep_val:,1] = 1.0\n",
        "\n",
        "#Alternate classes for better shuffle starting point\n",
        "buf_train_examples = np.copy(train_examples)\n",
        "buf_train_labels = np.copy(train_labels)\n",
        "\n",
        "buf_train_examples[::2] = train_examples[:class_count-nb_keep_val]\n",
        "buf_train_examples[1::2] = train_examples[class_count-nb_keep_val:]\n",
        "\n",
        "buf_train_labels[::2] = train_labels[:class_count-nb_keep_val]\n",
        "buf_train_labels[1::2] = train_labels[class_count-nb_keep_val:]\n",
        "\n",
        "train_examples = buf_train_examples\n",
        "train_labels = buf_train_labels\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.Resizing(image_size, image_size),\n",
        "  layers.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip('horizontal',\n",
        "        input_shape=(image_size, image_size, 3)),\n",
        "  layers.RandomRotation(factor=(-0.1, 0.1), fill_mode='constant'),\n",
        "  layers.RandomZoom(height_factor=(-0.2,0.2), width_factor=(-0.2,0.2), fill_mode='constant'),\n",
        "  layers.RandomContrast(0.2),\n",
        "  layers.RandomBrightness(0.2, value_range=(0.0, 1.0))\n",
        "])\n",
        "\n",
        "\n",
        "def prepare(ds, shuffle=False, augment=False):\n",
        "  ds = ds.map(lambda x, y: (resize_and_rescale(x), y),\n",
        "              num_parallel_calls=AUTOTUNE)\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(1000)\n",
        "\n",
        "  ds = ds.batch(batch_size)\n",
        "\n",
        "  if augment:\n",
        "    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y),\n",
        "                num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  return ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "train_dataset = prepare(train_dataset, shuffle=True, augment=True)\n",
        "test_dataset = prepare(test_dataset)\n",
        "\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "PaXxG3jKiBTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "total_iter = 10\n",
        "\n",
        "load_iter = 0\n",
        "\n",
        "if(load_iter > 0):\n",
        "\tmodel = models.models.load('%04d.keras'%(load_iter))\n",
        "else:\n",
        "\tmodel = keras.Sequential()\n",
        "\n",
        "\tmodel.add(layers.Conv2D(filters=16, kernel_size=(3, 3), padding='same'))\n",
        "\tmodel.add(layers.MaxPooling2D())\n",
        "\tmodel.add(layers.GroupNormalization(groups=8))\n",
        "\tmodel.add(layers.Activation('relu'))\n",
        "\n",
        "\tmodel.add(layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same'))\n",
        "\tmodel.add(layers.MaxPooling2D())\n",
        "\tmodel.add(layers.GroupNormalization(groups=16))\n",
        "\tmodel.add(layers.Activation('relu'))\n",
        "\n",
        "\tmodel.add(layers.Conv2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
        "\tmodel.add(layers.MaxPooling2D())\n",
        "\tmodel.add(layers.GroupNormalization(groups=16))\n",
        "\tmodel.add(layers.Activation('relu'))\n",
        "\n",
        "\tmodel.add(layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "\tmodel.add(layers.Conv2D(filters=64 , kernel_size=(1, 1), activation='relu', padding='same'))\n",
        "\tmodel.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
        "\tmodel.add(layers.MaxPooling2D())\n",
        "\tmodel.add(layers.GroupNormalization(groups=32))\n",
        "\tmodel.add(layers.Activation('relu'))\n",
        "\n",
        "\tmodel.add(layers.Conv2D(filters=192, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "\tmodel.add(layers.Conv2D(filters=128, kernel_size=(1, 1), activation='relu', padding='same'))\n",
        "\tmodel.add(layers.Conv2D(filters=192, kernel_size=(3, 3), padding='same'))\n",
        "\tmodel.add(layers.MaxPooling2D())\n",
        "\tmodel.add(layers.GroupNormalization(groups=32))\n",
        "\tmodel.add(layers.Activation('relu'))\n",
        "\n",
        "\tmodel.add(layers.Conv2D(filters=nb_class, kernel_size=(1, 1), padding='same'))\n",
        "\tmodel.add(layers.GlobalAveragePooling2D())\n",
        "\tmodel.add(layers.Activation('softmax'))\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "for run_iter in range(load_iter,total_iter):\n",
        "\n",
        "\tmodel.fit(train_dataset, batch_size=batch_size, epochs=1, shuffle=True, validation_data=test_dataset)\n",
        "\n",
        "\tmodel.save('%04d.keras'%(run_iter+1))\n",
        "\n",
        "\tpred = model.predict(test_dataset)\n",
        "\n",
        "\tmatrix = metrics.confusion_matrix(test_labels.argmax(axis=1), pred.argmax(axis=1))\n",
        "\tprint (matrix)\n",
        "\n",
        "#print(model.summary())\n",
        "\n"
      ],
      "metadata": {
        "id": "MvRhvumeRWZY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
